{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de877e0-9ca6-4329-80da-710a05e5a0e1",
   "metadata": {},
   "source": [
    "# Knowledge Graph\n",
    "\n",
    "In order to create the knowledge graph (KG) we are going to use spacy and the large english model. We will then use pyvis to visualise the network created by the algorithmn. The text data is just over 2.5M tokens so we will make the limit of the model 3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b07b980-73ff-4b42-a2d3-2b353bd67bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Load the large English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Initialize an empty graph\n",
    "graph = nx.Graph()\n",
    "\n",
    "limit = 3000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb9210-12d7-43e1-8161-90187ac307e2",
   "metadata": {},
   "source": [
    "Then we need to create functions that:\n",
    "\n",
    "1) Allow us to flex how much data to load in (for testing)\n",
    "2) Clean the data to remove page numbers, special characters and spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41244b08-4eef-427a-96d7-ebba738f3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to load text file\n",
    "def load_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to load a limited amount of text from the file\n",
    "def load_limited_text(file_path, limit=50000):\n",
    "    \"\"\"Load a maximum of 'limit' characters from the text file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        while len(text) < limit:\n",
    "            chunk = file.read(limit - len(text))  # Read only the needed remaining characters\n",
    "            if not chunk:  # If no more content to read, break the loop\n",
    "                break\n",
    "            text += chunk\n",
    "    return text\n",
    "\n",
    "# Function to clean text (remove HTML, special characters, and page numbers)\n",
    "def clean_text(text):\n",
    "    # Remove HTML entities\n",
    "    text = re.sub(r'&#[0-9]+;', '', text)\n",
    "\n",
    "    # Remove § and section numbers (e.g., §3)\n",
    "    text = re.sub(r'§\\d+', '', text)\n",
    "\n",
    "    # Remove standalone page numbers (e.g., \"531\", \"532\")\n",
    "    text = re.sub(r'\\b\\d{1,4}\\b', '', text)  # Match numbers with 1 to 4 digits\n",
    "\n",
    "    # Remove multiple newlines, excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02141c43-437f-4a01-b451-49e3f98a1855",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "#### Function Definition:\n",
    "- The function `add_to_graph(graph, doc)` takes two inputs:\n",
    "  - `graph`: The knowledge graph (could be a network graph like in NetworkX).\n",
    "  - `doc`: A document (likely processed by an NLP library like spaCy).\n",
    "\n",
    "#### Loop Through Sentences:\n",
    "- `for sent in doc.sents`: Loops through each sentence in the document.\n",
    "\n",
    "#### Extract Entities:\n",
    "- `entities_in_sentence = [ent for ent in sent.ents]`: Extracts **named entities** from each sentence and stores them in the list `entities_in_sentence`. These could be people, places, organizations, etc.\n",
    "\n",
    "#### Find the Root Verb (Relationship):\n",
    "- `root_verb = None`: Initializes the `root_verb` variable.\n",
    "- The loop `for token in sent` iterates over each token (word) in the sentence to find the **root verb** (main action/relationship in the sentence).\n",
    "- `token.dep_ == ROOT` checks if the token has the syntactic dependency ROOT, identifying the main verb/action in the sentence.\n",
    "- `root_verb = token.lemma_` stores the **lemma** (base form) of the verb as `root_verb`, which will be used as the **relationship** between entities.\n",
    "\n",
    "#### Create Edges Between Entities:\n",
    "- If a root verb is found (`if root_verb`) and there are at least two entities in the sentence (`len(entities_in_sentence) > 1`), the code creates edges (relationships) between those entities.\n",
    "- It loops through all pairs of entities (`ent1` and `ent2`) in the sentence. If the entities are different (`if ent1 != ent2`), it adds an **edge** to the graph using `graph.add_edge`.\n",
    "  - **Edge**: Connects `ent1.text` and `ent2.text` (the names of the entities).\n",
    "  - **Relationship**: The edge's relationship is represented by the `root_verb` (the verb from the sentence).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d9d5c16-682f-470b-8284-3934b62adafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_graph(graph, doc):\n",
    "    \"\"\"\n",
    "    Add entities to the graph and create edges based on the root verb and prepositional phrases in each sentence.\n",
    "    \"\"\"\n",
    "    # Entity types to include in the graph\n",
    "    valid_entity_types = {\"PERSON\", \"ORG\", \"GPE\", \"LOC\"}  # People, Organizations, Locations\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        # Extract valid entities from the sentence\n",
    "        entities_in_sentence = [ent for ent in sent.ents if ent.label_ in valid_entity_types]\n",
    "\n",
    "        # Find the root verb and possible prepositional phrase\n",
    "        root_verb = None\n",
    "        prep_phrase = None\n",
    "        modifiers = []  # To capture adverbs or adjectives that modify the verb\n",
    "\n",
    "        for token in sent:\n",
    "            if token.dep_ == \"ROOT\":\n",
    "                root_verb = token.lemma_\n",
    "            if token.dep_ == \"prep\":\n",
    "                prep_phrase = token.text\n",
    "            if token.dep_ in {\"advmod\", \"amod\"}:\n",
    "                modifiers.append(token.text)\n",
    "\n",
    "        # Combine root verb, prepositional phrase, and modifiers to form a more meaningful relationship\n",
    "        if root_verb:\n",
    "            # Build a detailed relationship string\n",
    "            relationship = f\"{root_verb} {' '.join(modifiers)} {prep_phrase}\".strip()\n",
    "\n",
    "            # Create edges only if there are more than one entity\n",
    "            if len(entities_in_sentence) > 1:\n",
    "                for i, ent1 in enumerate(entities_in_sentence):\n",
    "                    for ent2 in entities_in_sentence[i+1:]:\n",
    "                        if ent1 != ent2:\n",
    "                            # Lowercase entities for consistent graph entries\n",
    "                            entity1 = ent1.text.lower()\n",
    "                            entity2 = ent2.text.lower()\n",
    "\n",
    "                            # Avoid adding duplicate edges\n",
    "                            if not graph.has_edge(entity1, entity2):\n",
    "                                graph.add_edge(entity1, entity2, relationship=relationship)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd1a94-a5bc-41de-a335-310fdc53f080",
   "metadata": {},
   "source": [
    "We then load in our corpus of data, clean the text and check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19fca011-21d9-41fb-bbdf-120816ca206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE STORY AND AIM OF THE OUTLINE OF HISTORY T HE Outline of History was first written in -. It was published in illustrated parts, and it was carefully revised and printed again as a book in . It was again revised very severely and rearranged for a reprint in (January) ; it was reissued in a revised and much more amply illustrated edition in , and again in came a quite fresh edition, recast, rewritten in many places, and with much added new matter. This has now been further revised. There were many reasons to move a writer to attempt a World History in . It was the last, the weariest, most disillusioned year of the Great War. Everywhere there were unwonted privations ; everywhere there was mourning. The tale of the dead and mutilated had mounted to many millions. Men felt they had come to a crisis in the world’s affairs. They were too weary and heart-sick to consider complicated possibilities. They were not sure whether they were facing a disaster to civilization or the inauguration of\n"
     ]
    }
   ],
   "source": [
    "# Load the first 50,000 characters from your large text file and clean it\n",
    "text = load_limited_text(\"../data/raw/origin-of-the-world.txt\", limit=limit)\n",
    "text = clean_text(text)\n",
    "\n",
    "print(text[0:1000])\n",
    "\n",
    "# Increase spaCy's maximum document length limit\n",
    "nlp.max_length = len(text) + 1000  # Add some buffer to the current text length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1768db0c-dc71-4f35-8ffe-37136f2e2dbd",
   "metadata": {},
   "source": [
    "When we are happy with the output we can then process the documents. We use the command \"pipe\" to instigate parrallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e069947-7dcc-4702-ae6e-6bec5b00d5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Knowledge Graph:   0%|                                              | 1/1000 [18:20<305:30:29, 1100.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process the single chunk of text (since it's already 50,000 characters)\n",
    "docs = nlp.pipe([text], batch_size=1, n_process=16)  # Re-enabled 'parser' for verb/root detection\n",
    "\n",
    "# Process the document and build the graph\n",
    "for doc in tqdm(docs, total=1000, desc=\"Building Knowledge Graph\"):\n",
    "    add_to_graph(graph, doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2266a8c-61fe-416a-97e4-cd69f9eca711",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(graph, f\"../data/result/knowledge_graph_2.0_limit={limit}.gml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26e1c7-7794-4017-bdfd-177cfcf4b380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d33c5-7820-4dd4-a4dd-4f4c87a5cc38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "know_graph",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
